# https://mlabonne.github.io/blog/graphsage/
import math
import pickle
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from torch_geometric import nn
from torch_geometric.data import Batch
from torch_geometric.nn import SAGEConv, global_mean_pool
import torch.nn.functional as F
from matplotlib import pyplot as plt
from torch_geometric.utils import degree, to_networkx
from collections import Counter
from torch_geometric.loader import NeighborLoader
import pandas as pd
import networkx as nx
import torch
from torch_geometric.utils import from_networkx
import numpy as np
from sklearn.model_selection import train_test_split
import warnings
import matplotlib
from torch_geometric.loader import RandomNodeSampler

matplotlib.use('TkAgg')
np.random.seed(42)
torch.manual_seed(42)
warnings.filterwarnings('ignore')

# read the csv file into a pandas dataframe
df = pd.read_csv('my_filtered_dataset.csv')
most_pred_features = ['Price', 'grade', 'sqft_living', 'bathrooms', 'sqft_living15', 'sqft_above', 'dist_to_center']
df = df[most_pred_features]

# Define the columns we consider as features and the target
feature_cols = list(df.columns[1:])
print('Dit zijn onze features: ' + str(feature_cols))
target = ['Price']
print('Target: ' + str(target))

# load the pickled graph
with open('netw6_k30.pickle', 'rb') as f:
    G = pickle.load(f)

pyg_graph = from_networkx(G)

# G = max(nx.connected_component_subgraphs(G), key=len)
# Randomly split the nodes into training, validation, and test sets
nodes = list(G.nodes())
train_nodes, test_nodes = train_test_split(nodes, test_size=0.2, random_state=42)
train_nodes, val_nodes = train_test_split(train_nodes, test_size=0.25, random_state=42)
# Create empty masks for the training, test, and validation sets
train_mask = np.zeros(len(nodes), dtype=bool)
test_mask = np.zeros(len(nodes), dtype=bool)
val_mask = np.zeros(len(nodes), dtype=bool)
# Mark the nodes in the train_nodes, test_nodes, and val_nodes lists as True in their respective masks
train_mask[train_nodes] = True
test_mask[test_nodes] = True
val_mask[val_nodes] = True
# print('Train mask: ', train_mask)
# print('Test mask: ', test_mask)
# print('Validation mask: ', val_mask)
pyg_graph.train_mask = train_mask
pyg_graph.val_mask = val_mask
pyg_graph.test_mask = test_mask
# Standardizing our features:
scaler = StandardScaler()
scaler.fit(pyg_graph.x.float()[train_mask])
pyg_graph.x = torch.tensor(scaler.transform(pyg_graph.x.float()))

# Scaling the house prices:
scaler2 = MinMaxScaler()
scaler2.fit(pyg_graph.y.float()[train_mask])
pyg_graph.y = torch.tensor(scaler2.transform(pyg_graph.y.float()))

input_layers = len(pyg_graph.x[0])

# Create batches with neighbor sampling
train_loader = NeighborLoader(
    pyg_graph,
    num_neighbors=[8,16],
    batch_size=512,
)

val_loader = NeighborLoader(
    pyg_graph,
    num_neighbors=[8, 16],
    batch_size=256,  # smaller batch size for validation set
    shuffle=False,  # no need to shuffle validation set
)

test_loader = NeighborLoader(
    pyg_graph,
    num_neighbors=[8, 16],
    batch_size=256,
    shuffle=False,
)

# Print each subgraph
for i, subgraph in enumerate(train_loader):
    print(f'Subgraph {i}: {subgraph}')

class GraphSAGE(torch.nn.Module):
    """GraphSAGE"""

    def __init__(self, input_layers):
        super().__init__()
        self.sage1 = SAGEConv(input_layers, 256)
        self.sage2 = SAGEConv(256, 128)
        self.linear = nn.Linear(128, 1)
        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=5e-4)

    def forward(self, x, edge_index):
        h = self.sage1(x, edge_index).relu()
        h = F.dropout(h, p=0.6, training=self.training)
        h = self.sage2(h, edge_index)
        h = F.dropout(h, p=0.6, training=self.training)
        # print("h.shape", h.shape)
        h = self.linear(h)
        # print("na linear", h.shape)
        return h

    def fit(self, data, epochs):
        criterion = torch.nn.MSELoss()
        optimizer = self.optimizer

        self.train()
        val_losses = []
        train_losses = []
        for epoch in range(epochs + 1):
            total_train_mse = 0
            total_train_rmse = 0
            total_len_trainbatch = 0
            total_val_mse = 0
            total_val_rmse = 0
            total_len_valbatch = 0
            # Train on batches
            for batch in train_loader:
                optimizer.zero_grad()
                x = batch.x.to(torch.float32)
                #print('features: ',batch.x)
                out = self(x, batch.edge_index)
                y = batch.y.to(torch.float32)
                predicted_train = out[batch.train_mask]
                actual_train = y[batch.train_mask]
                train_loss = criterion(predicted_train, actual_train)
                nonscaled_actual_train = scaler2.inverse_transform(actual_train)
                nonscaled_actual_train = torch.Tensor(nonscaled_actual_train)
                #print('The actual training house prices are: ', nonscaled_actual_train[100:160])
                nonscaled_predicted_train = scaler2.inverse_transform(predicted_train.detach().numpy())
                nonscaled_predicted_train = torch.Tensor(nonscaled_predicted_train)
                #print('The predicted training house prices are: ', nonscaled_predicted_train)
                nonscaled_train_loss = criterion(nonscaled_predicted_train, nonscaled_actual_train)
                total_train_mse += nonscaled_train_loss * len(predicted_train)
                total_len_trainbatch += len(predicted_train)
                total_train_rmse += torch.sqrt(nonscaled_train_loss) * len(predicted_train)
                train_loss.backward()
                optimizer.step()

            train_losses.append(total_train_mse / total_len_trainbatch)

            # Evaluate on validation set
            with torch.no_grad():
                self.eval()
                for batch in val_loader:
                    x = batch.x.to(torch.float32)
                    out = self(x, batch.edge_index)
                    y = batch.y.to(torch.float32)
                    predicted_val = out[batch.val_mask]
                    actual_val = y[batch.val_mask]
                    nonscaled_actual_val = scaler2.inverse_transform(actual_val)
                    nonscaled_actual_val = torch.Tensor(nonscaled_actual_val)
                    nonscaled_predicted_val = scaler2.inverse_transform(predicted_val.detach().numpy())
                    nonscaled_predicted_val = torch.Tensor(nonscaled_predicted_val)
                    nonscaled_val_loss = criterion(nonscaled_predicted_val, nonscaled_actual_val)
                    total_val_mse += nonscaled_val_loss * len(predicted_val)
                    total_len_valbatch += len(predicted_val)
                    total_val_rmse += torch.sqrt(nonscaled_val_loss) * len(predicted_val)
                val_losses.append(total_val_mse / total_len_valbatch)

            # Print metrics every epoch
            print(f'Epoch {epoch + 1:03d}'
                  f'| RMSE on train set: {total_train_rmse / total_len_trainbatch:.2f}'
                  f'| RMSE on val set: {total_val_rmse / total_len_valbatch:.2f}')

        # Plot validation and train losses:
        # convert the tensors to a list of numerical values
        val_losses_list = [x.detach().numpy().tolist() for x in val_losses[1:]]
        train_losses_list = [x.detach().numpy().tolist() for x in train_losses[1:]]
        # plot the list of values
        # Create a list to store the epoch numbers
        epochsgraph = range(epochs)
        # Create a line plot of the validation loss values
        # print("val_loss:", val_loss)
        plt.plot(epochsgraph, val_losses_list, label='Validation Loss')
        plt.plot(epochsgraph, train_losses_list, label='Train Loss')
        # Add labels to the plot
        plt.title('Validation and Train Loss over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        # plt.legend()
        # Display the plot
        plt.show()


@torch.no_grad()
def test(model, data):
    """Evaluate the model on test set and print the RMSE score."""
    model.eval()
    total_test_mse = 0
    total_test_rmse = 0
    total_len_testbatch = 0
    for batch in test_loader:
        x = batch.x.to(torch.float32)
        out = model(x, batch.edge_index)
        y = batch.y.to(torch.float32)
        predicted_test = out[batch.test_mask]
        actual_test = y[batch.test_mask]
        nonscaled_actual_test = scaler2.inverse_transform(actual_test)
        nonscaled_actual_test = torch.Tensor(nonscaled_actual_test)
        nonscaled_predicted_test = scaler2.inverse_transform(predicted_test.detach().numpy())
        nonscaled_predicted_test = torch.Tensor(nonscaled_predicted_test)
        nonscaled_test_mse = torch.mean((nonscaled_predicted_test - nonscaled_actual_test) ** 2)
        total_test_mse += nonscaled_test_mse * len(predicted_test)
        total_len_testbatch += len(predicted_test)
        total_test_rmse += torch.sqrt(nonscaled_test_mse) * len(predicted_test)

    x = data.x.to(torch.float32)
    out = model(x, data.edge_index)
    actual_test_plt = data.y[data.test_mask]
    actual_test_plt = np.ravel(actual_test_plt)
    predicted_test_plt = out[data.test_mask]
    predicted_test_plt = np.ravel(predicted_test_plt)
    # Create the scatter plot
    plt.scatter(actual_test_plt, predicted_test_plt)
    # Add labels and title
    plt.xlabel('Actual test house price')
    plt.ylabel('Predicted test house price')
    plt.title('Actual vs Predicted House Prices')
    # Display the plot
    plt.axis('equal')
    plt.show()

    test_rmse = total_test_rmse / total_len_testbatch
    test_mse = total_test_mse / total_len_testbatch

    return test_rmse

# Create GraphSAGE
graphsage = GraphSAGE(input_layers)
print(graphsage)

# Train
graphsage.fit(pyg_graph, 50)

# Test
# valrmse = test(graphsage, pyg_graph, pyg_graph.val_mask)
testrmse = test(graphsage, pyg_graph)
# trainrmse = test(graphsage, pyg_graph, pyg_graph.train_mask)


# print(f'\n RMSE on validation set: {valrmse:.2f}\n')
print(f'\n RMSE on test set: {testrmse:.2f}\n')
